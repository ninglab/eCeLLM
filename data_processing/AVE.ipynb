{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "def load_pickle(addr):\n",
    "    with open(addr, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def dump_pickle(data, addr):\n",
    "    with open(addr, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_json(addr):\n",
    "    with open(addr, 'r') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def dump_json(data, addr):\n",
    "    with open(addr, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "def readjsonl(file):\n",
    "    with open(file, \"r\", encoding=\"utf8\") as f:\n",
    "        jsonlist = []\n",
    "        for item in jsonlines.Reader(f):\n",
    "            jsonlist.append(item)\n",
    "        return jsonlist\n",
    "    \n",
    "def concat_text(raw_text):\n",
    "    concated_text = ''\n",
    "    for s in raw_text:\n",
    "        concated_text += ' ' + s\n",
    "    return concated_text.strip()\n",
    "\n",
    "def is_similar(text, target):\n",
    "    if re.sub(r'[\\W_]+', '', text) == re.sub(r'[\\W_]+', '', target):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split(path):\n",
    "    data = {'train': [], 'val': [], 'test': []}\n",
    "    for key in data:\n",
    "        pos = readjsonl(os.path.join(path, key, '00_All/mave_positives.jsonl'))\n",
    "        neg = readjsonl(os.path.join(path, key, '00_All/mave_negatives.jsonl'))\n",
    "        data[key] = pos + neg\n",
    "    return data\n",
    "\n",
    "def add_attr(path):\n",
    "    data = read_split(path)\n",
    "    for key in data:\n",
    "        spl = data[key]\n",
    "        for entry in spl:\n",
    "            entry[\"attr_key\"] = entry[\"attributes\"][0][\"key\"]\n",
    "        data[key] = pd.DataFrame(spl)\n",
    "    return data\n",
    "\n",
    "def holdout_attr(df_test):\n",
    "    attrs = df_test[\"attr_key\"].unique().tolist()\n",
    "    random.shuffle(attrs)\n",
    "    random.shuffle(attrs)\n",
    "    holdouts = []\n",
    "    cnt = 0\n",
    "    for attr in attrs:\n",
    "        if cnt >= 1000:\n",
    "            break \n",
    "        df_holdout = df_test[df_test[\"attr_key\"] == attr]\n",
    "        cnt += len(df_holdout)\n",
    "        holdouts.append(attr)\n",
    "    return holdouts\n",
    "\n",
    "def filter_df(data):\n",
    "    holdouts = holdout_attr(data['test'])\n",
    "    data_holdouts = {'train': [], 'val': [], 'test': []}\n",
    "    for key in data:\n",
    "        df, df_woh = data[key], data[key]\n",
    "        df_wh = pd.DataFrame()\n",
    "        for h in holdouts:\n",
    "            df_woh = df_woh[df_woh[\"attr_key\"] != h]\n",
    "            df_wh = pd.concat([df_wh, df[df[\"attr_key\"] == h]])\n",
    "        data[key] = df_woh.sample(frac=1, random_state=seed)\n",
    "        data_holdouts[key] = df_wh.sample(frac=1, random_state=seed)\n",
    "    return data, data_holdouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_stru_data(df, size):\n",
    "    stru_data = []\n",
    "    for index, entry in tqdm(df.iterrows(), total=len(df)):\n",
    "        if len(stru_data) == size:\n",
    "            return stru_data\n",
    "        paragraphs = entry['paragraphs']\n",
    "        attributes = entry['attributes']\n",
    "        texts = {}\n",
    "        pids = {}\n",
    "        for i in range(len(paragraphs)):\n",
    "            para = paragraphs[i]\n",
    "            key = 'product ' + para['source']\n",
    "            if key not in texts:\n",
    "                texts[key] = []\n",
    "            texts[key].append(para['text'])\n",
    "            pids[i] = key\n",
    "        \n",
    "        for key in texts.keys():\n",
    "            texts[key] = ', '.join(texts[key])\n",
    "\n",
    "        attr_cand = []\n",
    "        pairs = set()\n",
    "        for attr in attributes:\n",
    "            attr_cand.append(attr['key'])\n",
    "            evids = attr['evidences']\n",
    "            if len(evids) == 0:\n",
    "                pairs.add((attr['key'].lower(), 'None', 'None'))\n",
    "            for evid in evids:\n",
    "                pairs.add((attr['key'].lower(), evid['value'].lower(), pids[evid['pid']].lower()))\n",
    "\n",
    "        new_entry = {}\n",
    "        new_entry[\"instruction\"] = \"Given the title, description, feature, price, and brand of a product and a set of target attributes, extract the value of each target attribute from the product information. Output the extracted value and the corresponding source (e.g., title or feature) denoting where the value is extracted.\"\n",
    "        texts['target attributes'] = ' and '.join(attr_cand)\n",
    "        new_entry[\"input\"] = json.dumps(texts)\n",
    "        new_entry[\"output\"] = json.dumps([{\n",
    "            'attribute': i[0],\n",
    "            'value': i[1],\n",
    "            'source': i[2]\n",
    "        } for i in pairs])\n",
    "        stru_data.append(new_entry)\n",
    "    return stru_data\n",
    "\n",
    "def process_df(data, train_size, vt_size, ood=False):\n",
    "    dir = './stru_data/attr_value_extraction'\n",
    "    if ood:\n",
    "        dir = './stru_data/attr_value_extraction_OOD'\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    for key in data:\n",
    "        if ood and key != 'test':\n",
    "            continue\n",
    "        size = vt_size    # size of test and validation set\n",
    "        if key == 'train':\n",
    "            size = train_size    # size of training set\n",
    "        stru_data = process_stru_data(data[key], size)\n",
    "        dump_json(stru_data, '{}/{}_{}k.json'.format(dir, key, len(stru_data)//1000))\n",
    "    return\n",
    "\n",
    "def process_ave(path, train_size, vt_size):\n",
    "    data = add_attr(path)\n",
    "    data, data_holdouts = filter_df(data)\n",
    "    process_df(data, train_size, vt_size)\n",
    "    process_df(data_holdouts, train_size, vt_size, ood=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "path = './datasets/mave/datasets/splits/PRODUCT'\n",
    "train_size, vt_size = 10000, 1000\n",
    "process_ave(path, train_size, vt_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diverse instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def diverse_instruction(path, instrs, unseen):\n",
    "    for file in os.listdir(path):\n",
    "        data = load_json(os.path.join(path, file))\n",
    "        for entry in data:\n",
    "            entry[\"instruction\"] = random.sample(instrs, k=1)[0]\n",
    "        if not os.path.exists(os.path.join('{}_di'.format(path))):\n",
    "            os.makedirs(os.path.join('{}_di'.format(path)))\n",
    "        dump_json(data, os.path.join('{}_di'.format(path), file))\n",
    "    \n",
    "        if file.startswith('test'):\n",
    "            for entry in data:\n",
    "                entry[\"instruction\"] = unseen\n",
    "            if not os.path.exists(os.path.join('{}_ui'.format(path))):\n",
    "                os.makedirs(os.path.join('{}_ui'.format(path)))\n",
    "            dump_json(data, os.path.join('{}_ui'.format(path), file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve the value associated with the target attribute from the product information and specify the source (e.g., title, description, feature, or title) where the value was found.\n",
      "['Extract the value of the target attribute from the given product information and output it along with the corresponding source.', 'Parse the product information to locate the target attribute, and then provide the extracted value of the target attribute and its source in the output, specifying None if the attribute is not present.', 'First, identify the target attributes from the provided list. Then, scan the product title, description, feature, and brand to extract the values associated with each target attribute. Finally, create a list of dictionaries, each containing the extracted attribute, its corresponding value, and the source where it was found.', \"Using the product's title, description, features, price, and brand, identify and retrieve the values associated with a specified set of target attributes. Output the extracted values along with their respective sources (e.g., title or feature) indicating where each value was found.\", 'Given the title, description, feature, price, and brand of a product and a set of target attributes, extract the value of each target attribute from the product information. Output the extracted value and the corresponding source (e.g., title or feature) denoting where the value is extracted.']\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "base_instr = \"Given the title, description, feature, price, and brand of a product and a set of target attributes, extract the value of each target attribute from the product information. Output the extracted value and the corresponding source (e.g., title or feature) denoting where the value is extracted.\"\n",
    "instrs = [\n",
    "\"Extract the value of the target attribute from the given product information and output it along with the corresponding source.\",\n",
    "\"Retrieve the value associated with the target attribute from the product information and specify the source (e.g., title, description, feature, or title) where the value was found.\",\n",
    "\"Parse the product information to locate the target attribute, and then provide the extracted value of the target attribute and its source in the output, specifying None if the attribute is not present.\",\n",
    "\"First, identify the target attributes from the provided list. Then, scan the product title, description, feature, and brand to extract the values associated with each target attribute. Finally, create a list of dictionaries, each containing the extracted attribute, its corresponding value, and the source where it was found.\",\n",
    "\"Using the product's title, description, features, price, and brand, identify and retrieve the values associated with a specified set of target attributes. Output the extracted values along with their respective sources (e.g., title or feature) indicating where each value was found.\"\n",
    "]\n",
    "unseen = random.sample(instrs, k=1)[0]\n",
    "instrs.remove(unseen)\n",
    "print(unseen)\n",
    "instrs.append(base_instr)\n",
    "print(instrs)\n",
    "diverse_instruction('./stru_data/attr_value_extraction', instrs, unseen)\n",
    "diverse_instruction('./stru_data/attr_value_extraction_OOD', instrs, unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def few_shot(path):\n",
    "    test_data = load_json('{}/test_1k.json'.format(path))\n",
    "    try:\n",
    "        train_data = load_json('{}/train_10k.json'.format(path))\n",
    "    except:\n",
    "        train_data = load_json('./stru_data/attr_value_extraction/train_10k.json')\n",
    "    few_shot = []\n",
    "    for index, entry in enumerate(test_data):\n",
    "        new_entry = {}\n",
    "        new_entry['instruction'] = entry['instruction']\n",
    "        new_entry['example'] = json.dumps({\n",
    "            'input': train_data[index]['input'],\n",
    "            'output': train_data[index]['output']\n",
    "        })\n",
    "        new_entry['test example'] = json.dumps({\n",
    "            'input': entry['input'],\n",
    "            'output': entry[\"output\"]\n",
    "        })\n",
    "        few_shot.append(new_entry)\n",
    "    if not os.path.exists('{}_few_shot'.format(path)):\n",
    "        os.makedirs('{}_few_shot'.format(path))\n",
    "    dump_json(few_shot, '{}_few_shot/test_1k.json'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot('./stru_data/attr_value_extraction')\n",
    "few_shot('./stru_data/attr_value_extraction_di')\n",
    "few_shot('./stru_data/attr_value_extraction_OOD')\n",
    "few_shot('./stru_data/attr_value_extraction_OOD_di')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
